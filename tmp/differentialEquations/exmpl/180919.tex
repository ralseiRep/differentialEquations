\Subsection{Лемма Фишера}

\begin{reminder}
	\slashns
	
	Матрица $C$ является ортогональной, если выполняется $C^TC = CC^T = E$, т.е. если $C^T = C^{-1}$.
\end{reminder}

\begin{statement}
	\slashns
	
	$X = (X_1, \, \ldots, \, X_n)^T, \; X_i$~--- независимые компоненты, $X_i \sim \N(0, \sigma^2)$.
	
	$C$~--- ортогональная матрица.
	
	Тогда $CX \overset{d}{=} X$.
	
	\begin{proof}
		\slashns
		
		$\E e^{i \angles{t, X}} = e^{-\frac{\sigma^2}{2} \angles{t, t}}$
		
		$\E e^{i \angles{t, CX}} = e^{-\frac{\sigma^2}{2} \angles{C^Tt, C^Tt}} = e^{-\frac{\sigma^2}{2} \angles{t, CC^Tt}} = e^{-\frac{\sigma^2}{2} \angles{t, t}}$
		
		Получили, что характеристические функции $X$ и $CX$ совпадают. А значит, по теореме единственности и сами распределения совпадают.
	\end{proof}
\end{statement}

\begin{definition}[Распределение хи-квадрат]
	\slashns
	
	$X_1, \, \ldots, \, X_n$~--- независимые случайные величины, $X_i \sim \N(0, 1)$.
	
	Тогда распределение хи-квадрат определяется как $\chi_n^2 = X_1^2 + \ldots + X_n^2$.
\end{definition}

\begin{definition}[Распределение Стьюдента]
	\slashns
	
	$X, \, X_1, \, \ldots, \, X_n$~--- независимые случайные величины, $X_i \sim \N(0, 1)$.
	
	Тогда распределение Стьюдента определяется как $T_n = \frac{X}{\sqrt{\frac1n (X_1^2 + \ldots + X_n^2)}}$.
\end{definition}

\begin{definition}
	\slashns
	
	Пусть $X_1, \, \ldots, \, X_n$~--- независимые наблюдения, $\overline{X} = \frac{X_1 + \ldots + X_n}{n}$~--- выборочное среднее.
	
	Тогда $S^2 = \frac1n \sum\limits_{k=1}^{n} (X_k - \overline{X})^2$ называется выборочной дисперсией.
	
	В частности, если раскрыть все скобки в вышеприведенной сумме, то можно получить следующее представление для выборочной дисперсии, аналогичное представлению для обычной дисперсии:
	$$S^2 = \left(\frac1n\sum\limits_{k=1}^{n} X_k^2 \right) - \overline{X}^2 \iff \D \xi = \E \xi^2 - \left(\E \xi\right)^2$$
\end{definition}

\begin{nmlemma}[Фишера]
	\slashns
	
	$X_1, \, \ldots, \, X_n$~--- независимые наблюдения, $X_i \sim \N(\theta, \sigma^2)$.
	
	$\overline{X} = \frac{X_1 + \ldots + X_n}{n}$
	
	$S^2 = \frac1n \sum_{k=1}^{n} (X_k - \overline{X})^2$
	
	Тогда:
	
	1) $\sqrt{n} \frac{\overline{X} - \theta}{\sigma} \sim \N(0, 1)$
	
	2) $\frac{nS^2}{\sigma^2} \sim \chi_{n-1}^2$
	
	3) $\overline{X}$ и $S^2$~--- независимы
	
	4) $\sqrt{n-1} \frac{\overline{X} - \theta}{S} \sim T_{n-1}$

	\begin{proof}
		\slashns
		
		\begin{enumerate}
			\item 
			$\overline{X}$~--- линейная комбинация независимых нормальных случайных величин, а значит имеет нормальное распределение.
			
			В этом легко убедиться, если взглянуть на характеристическую функцию $\overline{X}$.
			
			В частности, $\overline{X} \sim \N \left(\E \overline{X}, \D \overline{X}\right) = \N \left(\theta, \frac{\sigma^2}{n}\right)$.
			
			А значит, $\frac{\overline{X} - \theta}{\sqrt{\sigma^2/n}} \sim \N(0, 1)$.
			
			\item[2, 3.]  
			Можно считать, что $\theta = 0$.
			
			Действительно, если мы все наблюдения сдвинем на $\theta$, то $S^2$ никак не изменится.
			
			Тем не менее, $\overline{X}$ изменится. Но т.к. изменение происходит на константу, то на независимость $S^2$ и $\overline{X}$ это никак не повлияет.
			
			Рассмотрим тогда вектор $X = (X_1, \, \ldots, \, X_n)^T$, где $X_k$~--- независимы, $X_k \sim \N(0, \sigma^2)$.
			
			И рассмотрим ортогональную матрицу $C$ следующего вида:
			$$C = \begin{pmatrix}
			\frac{1}{\sqrt{n}} & \ldots & \frac{1}{\sqrt{n}}\\
			\cdot & \cdot & \cdot\\
			\cdot & \cdot & \cdot
			\end{pmatrix}$$
			Здесь важно, чтобы первая строка этой матрицы имела ровно такой вид. В частности, можно заметить, что такая матрица всегда существует.
			
			Тогда имеем $Y = CX \overset{d}{=} X$.
			
			Попытаемся выразить выборочное среднее и выборочную дисперсию через $Y_k$:
			
			$Y_1 = \frac{1}{\sqrt{n}}(X_1 + \ldots + X_n) = \sqrt{n} \cdot \overline{X} \implies \overline{X} = \frac{Y_1}{\sqrt{n}}$
			
			Распишем теперь выборочную дисперсию. Воспользуемся тем фактом, что $C$ не меняет длину вектора:
			
			$S^2 = \frac{1}{n} \sum\limits_{k=1}^{n} X_k^2 - \overline{X}^2 = \frac{1}{n} \sum\limits_{k=1}^{n} Y_k^2 - \frac{Y_1^2}{n} = \frac1n \sum\limits_{k=2}^{n} Y_k^2$
			
			Таким образом, мы доказали оба пункта.
			
			А именно, второй пункт получается элементарной подстановкой вместо $S^2$ полученного результата.
			
			Третий же пункт следует из того, что $\overline{X}$ и $S^2$ выражаются через разные $Y_k$, а потому независимы.
			
			\item[4.]
			$T_n = \frac{X}{\sqrt{\frac1n (X_1^2 + \ldots + X_n^2)}}$, где $X, X_i \sim \N(0, 1)$ и независимы.
				
			Положим $X = \frac{\overline{X} - \theta}{\sqrt{\sigma^2 / n}} = \sqrt{n} \frac{\overline{X} - \theta}{\sigma}$.
				
			Тогда имеем:
			
			$T_{n-1} \sim \frac{X}{\sqrt{\frac{1}{n-1} \chi_{n-1}^2}} = \dfrac{\sqrt{n} \frac{\overline{X} - \theta}{\sigma}}{\sqrt{\frac{1}{n-1} \cdot \frac{n S^2}{\sigma^2}}} = \sqrt{n-1} \frac{\overline{X} - \theta}{S}$
		\end{enumerate}
	\end{proof}
\end{nmlemma}

\Subsection{Построение оценок. Метод моментов}

\begin{motivation}
	\slashns
	
	До сих пор мы старались по имеющимся у нас наблюдениям восстановить информацию о распределении.
	
	К примеру, мы уже выяснили, что функция распределения неплохо приближается эмпирической функцией распределения, а матожидание~--- выборочным средним.
	
	Более того, таким же образом можно пробовать оценивать и различные функционалы (к примеру, интегральные):
	
	$\int g(x) \; d \F(x) = \int g(x) \; d \F_n(x) = \frac{1}{n} \sum\limits_{k=1}^{n} g(X_k)$
	
	Хотелось бы применить текущие знания для построения некоторого метода, который бы позволил хорошо приближать интересующие нас параметры.
\end{motivation}

\begin{problem}
	\slashns
	
	Вернемся к общей задаче. Будем рассматривать простой случай, когда все наблюдения и параметры существуют в $\R$.
	
	Пусть $X_1, \, \ldots, \, X_n \sim P_{\theta}$~--- наблюдения, $\theta \in \Theta$.
	
	Как и всегда, сам параметр $\theta$ нам неизвестен, но мы хотим его оценить.
\end{problem}

\begin{solution}
	\slashns
	
	Рассмотрим некоторую функцию $g : \R \to \R$, такую что генеральный момент $\E_{\theta} g(X) =: h(\theta)$~--- достаточно хорошая функция. На текущий момент будем требовать, чтобы $h$ была обратима.
	
	\vspace*{-1pt}
	Предположим при этом, что наш генеральный момент $\E_{\theta} g(X)$ достаточно хорошо приближается выборочным. На самом деле, в идеальном случае мы бы имели следующее равенство:
	
	\vspace*{-1pt}
	$\frac1n \sum\limits_{k=1}^{n} g(X_k) = \int g(x) \; d \F_n(x) = \int g(x) \; d \F_{\theta}(x) = \E_{\theta} g(X) = h(\theta)$
	
	Отсюда появляется идея оценивать параметр как $\overline{\theta} = h^{-1} \left(\frac1n \sum\limits_{k=1}^{n} g(X_k) \right)$.
	
	Такая идея действительно кажется вполне разумной. Тем не менее, требуется исследовать свойства, которыми обладает такая оценка параметров, а также проверить, как хорошо она работает на каких-нибудь известных распределениях. 
\end{solution}

\begin{remark}
	\slashns
	
	В качестве $g(x)$ обычно выбирают функции какого-нибудь простого и удобного вида. Самый типичный вариант~--- $g(x) = x^k$, поскольку в таком случае нам приходится работать с моментами $\E X^k$, а о них мы уже многое знаем.
\end{remark}

\begin{remark}
	\slashns
	
	Если параметр лежит в пространстве большей размерности (т.е. $\Theta \subset \R^d$, где $d > 1$), то такой подход по-прежнему имеет право на существование. Однако теперь, чтобы получить $\overline{\theta}$, нам потребуется по крайней мере $d$ различных уравнений.
\end{remark}

\begin{example}
	\slashns
	
	\begin{enumerate}
		\item
		Рассмотрим нормальное распределение $\N(\theta, \sigma^2)$.
		
		В данном случае мы имеем два неизвестных параметра. Для их оценки попробуем выписать первые два момента, т.е. выберем $g_1(x) = x$ и $g_2(x) = x^2$:
		
		$\begin{cases}
		\E_{(\overline{\theta}, \overline{\sigma})} X = \frac1n \sum\limits_{k=1}^{n} X_k\\
		\E_{(\overline{\theta}, \overline{\sigma})} X^2 = \frac1n \sum\limits_{k=1}^{n} X_k^2
		\end{cases} 
		\overset{\D X = \E X^2 - (\E X)^2}{\iff}
		\begin{cases}
		\overline{\theta} = \overline{X}\\
		\overline{\sigma}^2 + \overline{\theta}^2 = \frac1n \sum\limits_{k=1}^{n} X_k^2
		\end{cases}
		\iff
		\begin{cases}
		\overline{\theta} = \overline{X}\\
		\overline{\sigma}^2 = \left(\frac1n \sum\limits_{k=1}^{n} X_k^2\right) - \overline{X}^2 
		\end{cases}$
		
		Получили, что параметры нормального распределения можно оценить как $\overline{X}$ и $S^2$.
		
		Но мы уже знаем, что эти оценки хорошо приближают матожидание и дисперсию произвольного случайного распределения. 
		
		А так как параметрами нормального распределения как раз являются его матожидание и дисперсия, то мы действительно получили неплохую оценку его параметров. 
		
		Значит, есть надежда, что этот метод действительно работает неплохо.
		
		\item
		Другой пример~--- распределение Коши.
		
		Напомним, что распределение Коши~--- это класс абсолютно непрерывных распределений.
		
		Плотность таких распределений имеет вид $f(x) = \frac1\pi \left[\frac{1}{(x- \theta)^2 + 1} \right]$, где $\theta$~--- параметр сдвига.
		
		Это распределение примечательно тем, что оно не имеет математического ожидания. А потому оценивать это распределение с помощью первого момента просто не получится.
	\end{enumerate}
\end{example}

Теперь нам остается лишь проверить свойства, которыми обладают выведенные нами оценки.

Но прежде чем приступать к их рассмотрению, докажем следующее утверждение:

\begin{statement}
	\slashns
	
	$\sqrt{n} \frac{T_n - a}{\sigma} \to \N(0, 1)$
	
	$f$~--- непрерывная функция, $f'(a) \ne 0$
	
	Тогда $f(T_n)$ асимптотически нормальна.
	
	\begin{proof}
		\slashns
		
		%$f(T_n) = f(a) + (T_n - a) f'(T_n) + o(T_n - a)$
		$f(T_n) = f(a) + (T_n - a) f'(a) + o(T_n - a)$
		
		%$\sqrt{n} \left(f(T_n) - f(a) \right) = \sqrt{n} (T_n - a) f'(T_n) + o\left(\sqrt{n} (T_n - a) \right)$
		$\sqrt{n} \left(f(T_n) - f(a) \right) = \sqrt{n} (T_n - a) f'(a) + o\left(\sqrt{n} (T_n - a) \right)$
		
		%$\sqrt{n} (T_n - a) \overset{d}{\to} \N(0, \sigma^2)$, $f'(T_n) \overset{\P}{\to} f'(a)$
		$\sqrt{n} (T_n - a) \overset{d}{\to} \N(0, \sigma^2)$
		
		$o\left(\sqrt{n} (T_n - a) \right) = o(1) \cdot \sqrt{n} (T_n - a) \overset{d}{\to} 0$
		
		Т.е. $\sqrt{n} \left(f(T_n) - f(a) \right) \overset{d}{\to} \N(0, \sigma^2 \cdot \left(f'(a) \right)^2)$
	\end{proof}
\end{statement}

\begin{remark}
	\slashns
	
	Важно, что $f'(a)$ не обращается в ноль, поскольку иначе пришлось бы расписывать последующие члены в ряде Тейлора.
\end{remark}

\begin{properties}
	\slashns
	
	Поймем, что происходит со свойствами таких оценок:
	
	\vspace*{-10pt}
	\begin{enumerate}
		\item Несмещенность.
		
		Хотим проверить равенство $\E \overline{\theta} \overset{?}{=} \theta$.
		
		Положим $Z := \frac1n \sum\limits_{k=1}^{n} g(X_k)$. Тогда имеем:
		
		$\E \overline{\theta} = \E h^{-1}\left(\frac1n \sum\limits_{k=1}^{n} g(X_k) \right) = \E h^{-1}(Z)$
		
		$\theta = h^{-1}(h(\theta)) \overset{\text{def.}}{=} h^{-1}(\E g(X)) = h^{-1}\left(\frac1n \sum\limits_{k=1}^{n} \E g(X_k) \right) = h^{-1}\left(\E \left[\frac1n \sum\limits_{k=1}^{n} g(X_k) \right] \right) = h^{-1}(\E Z)$
		
		Таким образом, требуется проверить равенство $h^{-1}(\E Z)$ и $\E h^{-1}(Z)$. 
		
		Но если $h^{-1}$ строго выпукла или строго вогнута (а такая ситуация вполне обычная), то по неравенству Йенсена получается строгое неравенство.
		
		Значит, несмещенность в общем случае может и не существовать.
		
		\item Состоятельность.
		
		Если $\D g(X) < \infty$ и $h^{-1}$ непрерывна, то получаем:
		
		$\frac1n \sum\limits_{k=1}^{n} g(X_k) \overset{\P}{\to} \E_{\theta} g(X) = h(\theta)$ (закон больших чисел)
		
		$\overline{\theta} = h^{-1}\left(\frac1n \sum\limits_{k=1}^{n} g(X_k)\right) \overset{\P}{\to} h^{-1}(h(\theta)) = \theta$
	
		\item Асимптотическая нормальность.
		
		Если $\D g(X) < \infty$, а $h^{-1}$ непрерывно дифференцируема и не обращается в ноль, то получаем:
		
		$\dfrac{\frac1n \sum\limits_{k=1}^{n} g(X_k) - \E_{\theta} g(X)}{\sqrt{\frac{\D g(X)}{n}}} \overset{d}{\to} \N(0, 1)$ (центральная предельная теорема)
		
		Тогда, применяя к нашей оценке $h^{-1}$, по доказанному выше утверждению получаем:
		
		$\sqrt{n} (\overline{\theta} - \theta) \overset{d}{\to} \N \left(0, \D[g(X)] \cdot \left((h^{-1})' \circ h(\theta) \right)^2\right)$
	\end{enumerate}
\end{properties}



